"""
Maintenance helpers for simulator result and cache management.

This module centralises the logic used by ``run_final_analysis.py`` and
``run_master.py`` so both front-ends expose the same maintenance surface.
"""

from __future__ import annotations

import csv
import glob
import shutil
from dataclasses import dataclass, field
from pathlib import Path
from typing import Callable, Dict, List, Optional, Sequence, Tuple
from datetime import datetime
from typing import TextIO


TreePrinter = Callable[[str], None]


@dataclass(frozen=True)
class StageDefinition:
    """Metadata describing a resettable analysis stage/sweep."""

    id: str
    name: str
    description: str
    csv_patterns: Tuple[str, ...]
    cache_patterns: Tuple[str, ...]
    extra_patterns: Tuple[str, ...] = ()


# Stage numbering (referenced in run_final_analysis + run_master)
#   Stage 1 - SER vs Nm sweeps
#   Stage 2 - LoD vs distance generation
#   Stage 3 - Device FoM parameter sweeps
#   Stage 4 - Guard-frontier / ISI trade-off sweeps
#   Stage 5 - Main/publication figures
#   Stage 6 - Supplementary figures
#   Stage 7 - Tables and summary sheets
#   Stage 8 - Sensitivity sweeps (temp/diffusion/binding/device/correlation)
#   Stage 9 - Organoid sensitivity (alpha/bias/q_eff) sweeps
#   Stage 10 - Symbol-period (Ts) sweeps
#   Stage 11 - Capacity analysis exports

STAGE_REGISTRY: Dict[str, StageDefinition] = {
    "1": StageDefinition(
        id="1",
        name="SER vs Nm",
        description="Monte Carlo SER curves per modulation mode.",
        csv_patterns=("results/data/ser_vs_nm*.csv",),
        cache_patterns=(
            "results/cache/*/ser_vs_nm_*",
            "results/cache/*/ser_vs_nm*/**",
            "results/cache/*/nm_refine*/**",
            "results/cache/*/ser_refine*/**",
        ),
    ),
    "2": StageDefinition(
        id="2",
        name="LoD vs distance",
        description="Stage 13 LoD grid search and validation.",
        csv_patterns=("results/data/lod_vs_distance_*.csv",),
        cache_patterns=(
            "results/cache/*/lod_state/**",
            "results/cache/*/lod_search*/**",
            "results/cache/*/lod_validation*/**",
            "results/cache/*/lod_sigma*/**",
            "results/cache/*/d*um/**",
        ),
        extra_patterns=(
            "results/data/lod_vs_distance_*.meta.json",
            "results/data/lod_vs_distance_*.lock",
        ),
    ),
    "3": StageDefinition(
        id="3",
        name="Device FoM sweeps",
        description="OECT gm/C sweeps used for device figures of merit.",
        csv_patterns=("results/data/device_fom_*.csv",),
        cache_patterns=(
            "results/cache/*/device_fom*/**",
        ),
    ),
    "4": StageDefinition(
        id="4",
        name="Guard frontier / ISI trade-off",
        description="Guard-factor sweeps and derived ISI frontiers.",
        csv_patterns=(
            "results/data/guard_frontier_*.csv",
            "results/data/guard_tradeoff_*.csv",
        ),
        cache_patterns=(
            "results/cache/*/guard_frontier*/**",
            "results/cache/*/guard_tradeoff*/**",
            "results/cache/*/isi_tradeoff*/**",
        ),
    ),
    "5": StageDefinition(
        id="5",
        name="Main figures",
        description="Publication-ready figure exports generated by plotting scripts.",
        csv_patterns=(),
        cache_patterns=(),
        extra_patterns=(
            "results/figures/*.png",
            "results/figures/*.pdf",
            "results/figures/*.svg",
            "results/figures/notebook_replicas/**/*.png",
            "results/figures/notebook_replicas/**/*.pdf",
            "results/figures/notebook_replicas/**/*.svg",
            "results/figures/publication/**/*.png",
            "results/figures/publication/**/*.pdf",
        ),
    ),
    "6": StageDefinition(
        id="6",
        name="Supplementary figures",
        description="Supplementary/appendix figure exports.",
        csv_patterns=(),
        cache_patterns=(),
        extra_patterns=(
            "results/figures/supplementary/**/*.png",
            "results/figures/supplementary/**/*.pdf",
            "results/figures/supplementary/**/*.svg",
            "results/figures/appendix/**/*.png",
            "results/figures/appendix/**/*.pdf",
        ),
    ),
    "7": StageDefinition(
        id="7",
        name="Tables & summaries",
        description="Table/summary outputs (TeX/CSV) used in publications.",
        csv_patterns=(
            "results/data/table_*.csv",
            "results/data/summary_*.csv",
        ),
        cache_patterns=(),
        extra_patterns=(
            "results/tables/**/*.tex",
            "results/tables/**/*.csv",
            "results/tables/**/*.md",
        ),
    ),
    "8": StageDefinition(
        id="8",
        name="Sensitivity sweeps",
        description="Parameter sensitivity sweeps (T, D, binding, device gm/C, correlation).",
        csv_patterns=(
            "results/data/sensitivity_T_*.csv",
            "results/data/sensitivity_D_*.csv",
            "results/data/sensitivity_binding*.csv",
            "results/data/sensitivity_device*.csv",
            "results/data/sensitivity_corr*.csv",
        ),
        cache_patterns=(),
        extra_patterns=(
            "results/figures/fig_sensitivity_*.png",
        ),
    ),
    "9": StageDefinition(
        id="9",
        name="Organoid sensitivity sweeps",
        description="Organoid noise/ser/snr sweeps over alpha, bias, q_eff.",
        csv_patterns=(
            "results/data/organoid_*_sensitivity*.csv",
        ),
        cache_patterns=(),
        extra_patterns=(
            "results/figures/organoid_*.png",
        ),
    ),
    "10": StageDefinition(
        id="10",
        name="Ts sweeps",
        description="SNR/SER versus symbol-period sweeps.",
        csv_patterns=(
            "results/data/snr_vs_ts_*.csv",
        ),
        cache_patterns=(),
        extra_patterns=(
            "results/figures/fig_snr_vs_ts*.png",
            "results/figures/fig_snr_panels*.png",
        ),
    ),
    "11": StageDefinition(
        id="11",
        name="Capacity analysis",
        description="Capacity bounds/table and capacity figures.",
        csv_patterns=(
            "results/data/capacity_bounds*.csv",
        ),
        cache_patterns=(),
        extra_patterns=(
            "results/data/capacity_bounds_table.tex",
            "results/figures/fig_capacity_*.png",
        ),
    ),
}

STAGE_ALIASES: Dict[str, str] = {
    "ser": "1",
    "ser_vs_nm": "1",
    "nm": "1",
    "lod": "2",
    "lod_vs_distance": "2",
    "device": "3",
    "device_fom": "3",
    "guard": "4",
    "guard_frontier": "4",
    "isi": "4",
    "figure": "5",
    "figures": "5",
    "main_figures": "5",
    "supplementary": "6",
    "supp": "6",
    "tables": "7",
    "table": "7",
    "sensitivity": "8",
    "sensitivity_sweeps": "8",
    "organoid": "9",
    "organoid_sensitivity": "9",
    "ts": "10",
    "snr_vs_ts": "10",
    "capacity": "11",
    "capacity_analysis": "11",
}

TREE_CATEGORIES: Dict[str, Tuple[str, int]] = {
    "cache": ("results/cache", 3),
    "data": ("results/data", 2),
    "figures": ("results/figures", 2),
    "logs": ("results/logs", 1),
}

AVAILABLE_LIST_CATEGORIES = sorted(
    set(TREE_CATEGORIES.keys())
    | {"thresholds", "lod", "ser", "device", "guard", "stages", "cache-summary", "overview"}
)


@dataclass
class MaintenanceContext:
    project_root: Path
    dry_run: bool = False
    log_path: Optional[Path] = None
    _log_handle: Optional[TextIO] = field(init=False, default=None)

    def _ensure_log(self) -> None:
        if not self.log_path:
            return
        if self._log_handle is None:
            self.log_path.parent.mkdir(parents=True, exist_ok=True)
            self._log_handle = self.log_path.open("a", encoding="utf-8")
            stamp = datetime.now().isoformat(timespec="seconds")
            self._log_handle.write(f"\n# Maintenance session {stamp}\n")
            self._log_handle.flush()

    def record(self, message: str) -> None:
        if not self.log_path:
            return
        self._ensure_log()
        assert self._log_handle is not None
        self._log_handle.write(message + "\n")
        self._log_handle.flush()

    def delete_path(self, path: Path, verbose: bool = True) -> bool:
        if not path.exists():
            return False
        display = path
        try:
            display = path.relative_to(self.project_root)
        except ValueError:
            pass
        if self.dry_run:
            if verbose:
                print(f"  [dry-run] would remove {display}")
            self.record(f"dry-run -> remove {display}")
            return True
        try:
            if path.is_dir():
                shutil.rmtree(path)
            else:
                path.unlink()
            if verbose:
                print(f"  removed {display}")
            self.record(f"removed {display}")
            return True
        except Exception as exc:  # pragma: no cover - defensive
            print(f"  failed to remove {display}: {exc}")
            self.record(f"failed to remove {display}: {exc}")
            return False

    def cleanup_empty_dirs(self, path: Path, stop: Path) -> None:
        if self.dry_run:
            return
        try:
            current = path
            while current != stop and current.is_dir():
                if any(current.iterdir()):
                    break
                display = current
                try:
                    display = current.relative_to(self.project_root)
                except ValueError:
                    pass
                current.rmdir()
                self.record(f"removed empty directory {display}")
                current = current.parent
        except FileNotFoundError:
            return

    def close(self) -> None:
        if self._log_handle is not None:
            self._log_handle.close()
            self._log_handle = None


def _print(msg: str, out: TreePrinter) -> None:
    out(msg)


def list_stage_catalog(out: TreePrinter = print) -> None:
    """Emit stage numbering summary."""
    _print("Stage catalogue:", out)
    for stage_id in sorted(STAGE_REGISTRY.keys(), key=int):
        stage = STAGE_REGISTRY[stage_id]
        _print(f"  [{stage.id}] {stage.name} - {stage.description}", out)


def _print_tree(base: Path, out: TreePrinter, max_depth: int = 2) -> None:
    if not base.exists():
        _print(f"  {base} (missing)", out)
        return

    def _walk(path: Path, depth: int) -> None:
        indent = "  " * depth
        label = f"{path.name}/" if path.is_dir() else path.name
        _print(f"{indent}- {label}", out)
        if not path.is_dir() or depth >= max_depth:
            return
        try:
            children = sorted(child for child in path.iterdir() if not child.name.startswith("."))
        except FileNotFoundError:
            return
        for child in children:
            _walk(child, depth + 1)

    _walk(base, 0)


def _list_thresholds(project_root: Path, out: TreePrinter) -> None:
    cache_root = project_root / "results" / "cache"
    paths = sorted(cache_root.glob("thresholds_*.json"))
    _print("Threshold cache files:", out)
    if not paths:
        _print("  <none>", out)
        return
    for path in paths:
        _print(f"  {path.relative_to(project_root)}", out)


def _list_lod(project_root: Path, out: TreePrinter) -> None:
    data_root = project_root / "results" / "data"
    files = sorted(data_root.glob("lod_vs_distance_*.csv"))
    _print("LoD vs distance datasets:", out)
    if not files:
        _print("  <none>", out)
        return
    for csv_path in files:
        distances: List[str] = []
        try:
            with csv_path.open("r", encoding="utf-8", newline="") as fh:
                reader = csv.DictReader(fh)
                for row in reader:
                    value = row.get("distance_um")
                    if value is None:
                        continue
                    try:
                        distances.append(f"{float(value):.0f}")
                    except ValueError:
                        continue
        except FileNotFoundError:
            continue
        summary = ", ".join(sorted(set(distances)))
        _print(f"  {csv_path.relative_to(project_root)} -> [{summary}]", out)


def _list_ser(project_root: Path, out: TreePrinter) -> None:
    data_root = project_root / "results" / "data"
    files = sorted(data_root.glob("ser_vs_nm*.csv"))
    _print("SER vs Nm datasets:", out)
    if not files:
        _print("  <none>", out)
        return
    for csv_path in files:
        values: List[str] = []
        try:
            with csv_path.open("r", encoding="utf-8", newline="") as fh:
                reader = csv.DictReader(fh)
                for row in reader:
                    nm_val = None
                    for key in ("pipeline_Nm_per_symbol", "pipeline.Nm_per_symbol", "Nm", "Nm_per_symbol"):
                        if key in row and row[key]:
                            nm_val = row[key]
                            break
                    if nm_val is None:
                        continue
                    try:
                        values.append(f"{float(nm_val):.0f}")
                    except ValueError:
                        continue
        except FileNotFoundError:
            continue
        summary = ", ".join(sorted(set(values)))
        _print(f"  {csv_path.relative_to(project_root)} -> [{summary}]", out)


def _list_device(project_root: Path, out: TreePrinter) -> None:
    data_root = project_root / "results" / "data"
    files = sorted(data_root.glob("device_fom_*.csv"))
    _print("Device FoM datasets:", out)
    if not files:
        _print("  <none>", out)
        return
    for csv_path in files:
        _print(f"  {csv_path.relative_to(project_root)}", out)


def _list_guard(project_root: Path, out: TreePrinter) -> None:
    data_root = project_root / "results" / "data"
    trade = sorted(data_root.glob("guard_tradeoff_*.csv"))
    front = sorted(data_root.glob("guard_frontier_*.csv"))
    _print("Guard trade-off datasets:", out)
    if not trade:
        _print("  tradeoff: <none>", out)
    else:
        for path in trade:
            _print(f"  tradeoff: {path.relative_to(project_root)}", out)
    if not front:
        _print("  frontier: <none>", out)
    else:
        for path in front:
            _print(f"  frontier: {path.relative_to(project_root)}", out)


def _list_cache_summary(project_root: Path, out: TreePrinter) -> None:
    cache_root = project_root / "results" / "cache"
    _print("Cache summary (per mode):", out)
    if not cache_root.exists():
        _print("  <cache directory missing>", out)
        return
    for mode_dir in sorted(child for child in cache_root.iterdir() if child.is_dir()):
        _print(f"  {mode_dir.relative_to(project_root)}/", out)
        sweeps = sorted(child.name for child in mode_dir.iterdir() if child.is_dir())
        if not sweeps:
            _print("    <empty>", out)
            continue
        for sweep in sweeps:
            _print(f"    - {sweep}", out)


def list_resources(project_root: Path, categories: Sequence[str], ctx: Optional[MaintenanceContext] = None, out: TreePrinter = print) -> None:
    if not categories:
        _print("No maintenance categories provided.", out)
        return
    for category in categories:
        cat = category.lower()
        if cat in TREE_CATEGORIES:
            rel, depth = TREE_CATEGORIES[cat]
            base = project_root / rel
            _print(f"[{cat}] {base}", out)
            _print_tree(base, out, max_depth=depth)
        elif cat in ("thresholds", "threshold"):
            _list_thresholds(project_root, out)
        elif cat in ("lod", "distance", "distances"):
            _list_lod(project_root, out)
        elif cat in ("ser", "nm", "ser_vs_nm"):
            _list_ser(project_root, out)
        elif cat in ("device", "device_fom"):
            _list_device(project_root, out)
        elif cat in ("guard", "isi", "guard_frontier"):
            _list_guard(project_root, out)
        elif cat in ("stages", "stage"):
            list_stage_catalog(out)
        elif cat in ("cache-summary", "overview"):
            _list_cache_summary(project_root, out)
        else:
            _print(f"[{cat}] Unknown category. Available: {', '.join(AVAILABLE_LIST_CATEGORIES)}", out)
        if ctx is not None:
            ctx.record(f"listed maintenance category {cat}")


def _expand_patterns(project_root: Path, patterns: Sequence[str]) -> List[Path]:
    results: List[Path] = []
    for pattern in patterns:
        full_pattern = project_root / pattern
        results.extend(Path(p) for p in glob.glob(str(full_pattern), recursive=True))
    return results


def clear_threshold_cache(project_root: Path, ctx: MaintenanceContext, modes: Optional[Sequence[str]] = None, verbose: bool = True) -> int:
    cache_root = project_root / "results" / "cache"
    matches = [path for path in cache_root.glob("thresholds_*.json")]
    if modes:
        tokens = {token.lower() for token in modes if token and token.lower() != "all"}
        if tokens:
            matches = [path for path in matches if any(token in path.name.lower() for token in tokens)]
    removed = sum(1 for path in matches if ctx.delete_path(path, verbose=verbose))
    if verbose:
        print(f"Cleared {removed} threshold cache file(s).")
    return removed


def _iter_mode_dirs(project_root: Path, modes: Optional[Sequence[str]] = None) -> List[Path]:
    cache_root = project_root / "results" / "cache"
    if not cache_root.exists():
        return []
    requested = {m.lower() for m in modes} if modes else None
    dirs: List[Path] = []
    for mode_dir in sorted(child for child in cache_root.iterdir() if child.is_dir()):
        if requested and "all" not in requested and mode_dir.name.lower() not in requested:
            continue
        dirs.append(mode_dir)
    return dirs


def clear_lod_state(project_root: Path, specs: Sequence[str], ctx: MaintenanceContext, verbose: bool = True) -> int:
    if not specs:
        return 0
    normalized = [spec.lower() for spec in specs]
    wipe_all = any(token in ("all", "*") for token in normalized)
    removed = 0
    cache_root = project_root / "results" / "cache"
    if wipe_all:
        patterns = [
            "results/cache/*/lod_state/**",
            "results/cache/*/d*um",
            "results/cache/*/lod_search*/**",
            "results/cache/*/lod_validation*/**",
            "results/cache/*/lod_sigma*/**",
        ]
        for path in _expand_patterns(project_root, patterns):
            removed += ctx.delete_path(path, verbose=verbose)
        if verbose:
            print(f"Cleared LoD state for all modes (removed {removed} item(s)).")
        return removed

    targets: List[Tuple[Optional[str], Optional[int], Optional[str]]] = []
    for spec in specs:
        parts = spec.split(":")
        if len(parts) == 1:
            try:
                targets.append((None, int(float(parts[0])), None))
            except ValueError:
                targets.append((parts[0], None, None))
        else:
            mode = parts[0] or None
            dist = None
            ctrl = None
            try:
                dist = int(float(parts[1]))
            except ValueError:
                pass
            if len(parts) >= 3:
                ctrl = parts[2].lower()
            targets.append((mode, dist, ctrl))

    mode_filters = sorted({mode for mode, _, _ in targets if mode})
    mode_sequence: Optional[Sequence[str]] = mode_filters if mode_filters else None
    for mode_dir in _iter_mode_dirs(project_root, mode_sequence):
        for mode, dist, ctrl in targets:
            if mode and mode_dir.name.lower() != mode.lower():
                continue
            ctrl_candidates = []
            ctrl_token = (ctrl or "").lower()
            if ctrl_token in ("", "both"):
                ctrl_candidates = ["wctrl", "noctrl", "ctrl_unspecified"]
            elif ctrl_token.startswith("w"):
                ctrl_candidates = ["wctrl"]
            elif ctrl_token.startswith("n"):
                ctrl_candidates = ["noctrl"]
            else:
                ctrl_candidates = ["wctrl", "noctrl"]

            lod_state_root = mode_dir / "lod_state"
            for ctrl_seg in ctrl_candidates:
                state_dir = lod_state_root / ctrl_seg
                if dist is None:
                    removed += ctx.delete_path(state_dir, verbose=verbose)
                else:
                    state_file = state_dir / f"d{dist}um_state.json"
                    removed += ctx.delete_path(state_file, verbose=verbose)
                    ctx.cleanup_empty_dirs(state_file.parent, lod_state_root.parent)

            d_tag = f"d{dist}um" if dist is not None else None
            for child in mode_dir.iterdir():
                if not child.is_dir():
                    continue
                if dist is None and child.name.startswith("d") and child.name.endswith("um"):
                    removed += ctx.delete_path(child, verbose=verbose)
                elif d_tag and child.name == d_tag:
                    removed += ctx.delete_path(child, verbose=verbose)
    if verbose:
        print(f"Cleared {removed} LoD cache artefact(s).")
    return removed


def clear_seed_cache(project_root: Path, sweeps: Sequence[str], ctx: MaintenanceContext, modes: Optional[Sequence[str]] = None, verbose: bool = True) -> int:
    if not sweeps:
        return 0
    normalized = [token.lower() for token in sweeps]
    remove_all = any(token in ("all", "*") for token in normalized)
    removed = 0
    if remove_all:
        for mode_dir in _iter_mode_dirs(project_root, modes):
            removed += ctx.delete_path(mode_dir, verbose=verbose)
        if verbose:
            print(f"Cleared all per-mode seed caches (removed {removed} directories/files).")
        return removed

    for mode_dir in _iter_mode_dirs(project_root, modes):
        for sweep in normalized:
            pattern = f"**/{sweep}_*"
            for target in mode_dir.glob(pattern):
                if target.is_dir():
                    removed += ctx.delete_path(target, verbose=verbose)
    if verbose:
        print(f"Cleared seed cache entries matching sweeps {', '.join(sweeps)} (removed {removed}).")
    return removed


def _remove_csv_rows(csv_path: Path, column_candidates: Sequence[str], values: Sequence[float], verbose: bool = True) -> int:
    if not csv_path.exists():
        return 0
    try:
        with csv_path.open("r", encoding="utf-8", newline="") as fh:
            reader = csv.DictReader(fh)
            rows = list(reader)
            fieldnames = reader.fieldnames or []
    except Exception:  # pragma: no cover - CSV may be malformed
        return 0
    if not rows:
        return 0
    column = None
    for candidate in column_candidates:
        if candidate in rows[0]:
            column = candidate
            break
    if column is None:
        return 0
    targets = {float(value) for value in values}
    filtered: List[Dict[str, str]] = []
    for row in rows:
        try:
            val = float(row.get(column, "nan"))
        except ValueError:
            filtered.append(row)
            continue
        if val in targets:
            continue
        filtered.append(row)
    if len(filtered) == len(rows):
        return 0
    with csv_path.open("w", encoding="utf-8", newline="") as fh:
        writer = csv.DictWriter(fh, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(filtered)
    if verbose:
        print(f"  updated {csv_path} (removed {len(rows) - len(filtered)} row(s))")
    return len(rows) - len(filtered)


def clear_distance(project_root: Path, distances: Sequence[float], ctx: MaintenanceContext, modes: Optional[Sequence[str]] = None, verbose: bool = True) -> int:
    if not distances:
        return 0
    removed = 0
    int_distances = [int(float(d)) for d in distances]
    cache_root = project_root / "results" / "cache"
    for mode_dir in _iter_mode_dirs(project_root, modes):
        for dist in int_distances:
            d_tag = f"d{dist}um"
            target = mode_dir / d_tag
            removed += ctx.delete_path(target, verbose=verbose)
            lod_state_dir = mode_dir / "lod_state"
            for ctrl in ("wctrl", "noctrl", "ctrl_unspecified"):
                state_file = lod_state_dir / ctrl / f"{d_tag}_state.json"
                removed += ctx.delete_path(state_file, verbose=verbose)
                ctx.cleanup_empty_dirs(state_file.parent, lod_state_dir.parent)
    if cache_root.exists():
        ctx.cleanup_empty_dirs(cache_root, cache_root)

    data_root = project_root / "results" / "data"
    for csv_path in sorted(data_root.glob("lod_vs_distance_*.csv")):
        removed += _remove_csv_rows(csv_path, ("distance_um",), int_distances, verbose=verbose)
    if verbose:
        print(f"Processed LoD cleanup for distances {', '.join(str(d) for d in int_distances)}.")
    return removed


def clear_nm(project_root: Path, nm_values: Sequence[float], ctx: MaintenanceContext, modes: Optional[Sequence[str]] = None, verbose: bool = True) -> int:
    if not nm_values:
        return 0
    ints = [int(float(v)) for v in nm_values]
    removed = 0
    for mode_dir in _iter_mode_dirs(project_root, modes):
        for sweep_dir in mode_dir.glob("ser_vs_nm_*"):
            if not sweep_dir.is_dir():
                continue
            for nm_val in ints:
                key = f"{float(nm_val):.10g}"
                target = sweep_dir / key
                removed += ctx.delete_path(target, verbose=verbose)
            ctx.cleanup_empty_dirs(sweep_dir, mode_dir)
    data_root = project_root / "results" / "data"
    for csv_path in sorted(data_root.glob("ser_vs_nm*.csv")):
        removed += _remove_csv_rows(
            csv_path,
            ("pipeline_Nm_per_symbol", "pipeline.Nm_per_symbol", "Nm_per_symbol", "Nm"),
            ints,
            verbose=verbose,
        )
    if verbose:
        print(f"Processed SER cleanup for Nm values {', '.join(str(v) for v in ints)}.")
    return removed


def reset_stage(project_root: Path, stage_tokens: Sequence[str], ctx: MaintenanceContext, verbose: bool = True) -> int:
    if not stage_tokens:
        return 0
    resolved: List[str] = []
    for token in stage_tokens:
        token_lower = token.lower()
        if token_lower in STAGE_REGISTRY:
            resolved.append(token_lower)
        elif token_lower in STAGE_ALIASES:
            resolved.append(STAGE_ALIASES[token_lower])
        else:
            raise ValueError(f"Unknown stage identifier '{token}'.")
    removed = 0
    for stage_id in sorted(set(resolved), key=int):
        stage = STAGE_REGISTRY[stage_id]
        if verbose:
            print(f"Resetting stage {stage.id} - {stage.name}")
        patterns = stage.csv_patterns + stage.cache_patterns + stage.extra_patterns
        for path in _expand_patterns(project_root, patterns):
            removed += ctx.delete_path(path, verbose=verbose)
    return removed


def nuke_results(project_root: Path, ctx: MaintenanceContext, verbose: bool = True) -> None:
    results_root = project_root / "results"
    if not results_root.exists():
        if verbose:
            print("results/ directory does not exist - nothing to remove.")
        ctx.record("results/ directory missing; nothing to remove")
        return
    for child in list(results_root.iterdir()):
        ctx.delete_path(child, verbose=verbose)
    if verbose:
        print("Cleared entire results/ tree.")
    ctx.record("cleared entire results/ tree")


def execute_maintenance_flags(args, project_root: Path, out: TreePrinter = print) -> bool:
    """Interpret maintenance-related argparse flags."""
    list_categories = getattr(args, "list_maintenance", None)
    actions_requested = any([
        getattr(args, "nuke_results", False),
        bool(list_categories),
        bool(getattr(args, "list_stages", False)),
        getattr(args, "clear_threshold_cache", None) is not None,
        getattr(args, "clear_lod_state", None) is not None,
        getattr(args, "clear_seed_cache", None) is not None,
        getattr(args, "clear_distance", None) is not None,
        getattr(args, "clear_nm", None) is not None,
        bool(getattr(args, "reset_stage", None)),
    ])

    if not actions_requested:
        return False

    dry_run = bool(getattr(args, "maintenance_dry_run", False))
    log_override = getattr(args, "maintenance_log", None)
    log_path: Optional[Path]
    if isinstance(log_override, str):
        if log_override.strip().lower() in {"none", "-"}:
            log_path = None
        else:
            log_path = Path(log_override)
    else:
        log_path = log_override
    if log_path is None and not dry_run:
        log_path = project_root / "results" / "logs" / "maintenance.log"
    elif log_path is not None and not log_path.is_absolute():
        log_path = project_root / log_path

    ctx = MaintenanceContext(project_root=project_root, dry_run=dry_run, log_path=log_path)
    if dry_run:
        ctx.record("maintenance dry-run session started")
    else:
        ctx.record("maintenance session started")

    performed = False

    if getattr(args, "nuke_results", False):
        nuke_results(project_root, ctx, verbose=True)
        performed = True

    if list_categories:
        list_resources(project_root, list_categories, ctx=ctx, out=out)
        performed = True

    if getattr(args, "list_stages", False):
        list_stage_catalog(out=out)
        ctx.record("listed stage catalogue")
        performed = True

    thresh_modes = getattr(args, "clear_threshold_cache", None)
    if thresh_modes is not None:
        modes = thresh_modes if thresh_modes else None
        clear_threshold_cache(project_root, ctx, modes=modes, verbose=True)
        performed = True

    lod_specs = getattr(args, "clear_lod_state", None)
    if lod_specs is not None:
        spec_list = lod_specs or ["all"]
        clear_lod_state(project_root, spec_list, ctx, verbose=True)
        performed = True

    seed_specs = getattr(args, "clear_seed_cache", None)
    if seed_specs is not None:
        sweep_list = seed_specs or ["all"]
        clear_seed_cache(project_root, sweep_list, ctx, verbose=True)
        performed = True

    distances = getattr(args, "clear_distance", None)
    if distances is not None:
        clear_distance(project_root, distances, ctx, verbose=True)
        performed = True

    nm_values = getattr(args, "clear_nm", None)
    if nm_values is not None:
        clear_nm(project_root, nm_values, ctx, verbose=True)
        performed = True

    stage_tokens = getattr(args, "reset_stage", None)
    if stage_tokens:
        reset_stage(project_root, stage_tokens, ctx, verbose=True)
        performed = True

    if performed:
        ctx.record("maintenance actions completed")
    else:
        ctx.record("no maintenance action executed")

    ctx.close()
    return performed
